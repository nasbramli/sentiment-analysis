# -*- coding: utf-8 -*-
"""SentimentAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16iDdQ2mOhyaaxgJt3-YgkhcEHNWDPgCa
"""

from google.colab import drive
drive.mount('/content/drive/')

!mkdir dataset

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !cd dataset
# !unzip '/content/drive/MyDrive/Python Data/TSA/imdb_movie_review.zip' -d '/content/dataset'

path_csv = '/content/dataset/IMDB Dataset.csv'

import pandas as pd

df = pd.read_csv(path_csv)

df.head()

df.info()

# we have to remove unwanted words and other characters to improve the
# model performace.

import re
import nltk
from nltk.corpus import stopwords
import string

stemmer = nltk.SnowballStemmer('english')

nltk.download('stopwords')

stopword = set(stopwords.words('english'))

# Definig a Function to remove the unwanted words by using regex to search
# and replace it with ''
def clean_review(text):
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = [word for word in text.split(' ') if word not in stopword]
    text=" ".join(text)
    text = [stemmer.stem(word) for word in text.split(' ')]
    text=" ".join(text)
    return text

df["review"] = df["review"].apply(clean_review)

df.head()

"""# Train and Test Data Split"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

x = np.array(df['review'])
y = np.array(df['sentiment'])

cv = CountVectorizer()
X = cv.fit_transform(x)

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.20,
                                                    random_state=9)

y_train = [1 if i == 'positive' else 0 for i in y_train]
y_test = [1 if i == 'positive' else 0 for i in y_test]

import tensorflow as tf

def convert_sparse_matrix_to_sparse_tensor(X):
    coo = X.tocoo()
    indices = np.mat([coo.row, coo.col]).transpose()
    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))

X_train = convert_sparse_matrix_to_sparse_tensor(X_train)
X_test = convert_sparse_matrix_to_sparse_tensor(X_test)

"""# Vizulatizing Data"""

X_train.shape

X_test.shape

y_train[:5]

y_test[:5]

"""# Building Model"""

import tensorflow as tf

input_shape = (X_train.shape[1],)

def build_model(input_shape):

  model = tf.keras.models.Sequential()
  model.add(tf.keras.layers.Input(shape=input_shape, batch_size=32))
  model.add(tf.keras.layers.Dense(units=100, activation='relu'))
  model.add(tf.keras.layers.Dropout(0.2))
  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

  opt = tf.keras.optimizers.Adam()
  model.compile(optimizer=opt,
                loss='BinaryCrossentropy',
                metrics=['accuracy'])

  model.summary()

  return model

tsa_model = build_model(input_shape)

stop_early = tf.keras.callbacks.EarlyStopping(
                            monitor='val_loss', patience=5)
checkpoint = tf.keras.callbacks.ModelCheckpoint(
                        'tsa_model.h5',
                        verbose=1,
                        save_best_only=True,
                        monitor='val_loss')

# X_train, X_test, y_train, y_test

history = tsa_model.fit(x=X_train,y= np.array(y_train),
              validation_data=(X_test, np.array(y_test)),
              epochs=1000,
              callbacks=[stop_early, checkpoint])

# 100X1, Dropout(0.2) - 89.23% # Final Model
# 100X1, Dropout(0.4) - 88.54%

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(len(history.history['loss']))

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

print(f'the Accuracy of the model is {max(val_acc)}')

"""# Data Augmentation"""

!pip install textattack

from textattack.augmentation import WordNetAugmenter

# %%capture
from textattack.augmentation import WordNetAugmenter

wordnet_aug = WordNetAugmenter()

new_review, new_sentiment = [] , []
for i in df.index:
  if i < 5:
    new_review.append(wordnet_aug.augment(df['review'].iloc[i]))

new_review[1]

df.review[1]

df.sentiment[1]

a = cv.transform(new_review[1])
a = convert_sparse_matrix_to_sparse_tensor(a)
if tsa_model.predict(a)[0][0] == 1:
  print('positive')
else:
  print('negative')

# This shows that Using the above base code you can augment to generate new data
# which have synonyms of word of replaced words.
# above example is just to illustrate even after augmentation,
# The sentence is still positive.